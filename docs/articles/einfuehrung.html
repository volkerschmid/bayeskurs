<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Einführung in die Bayes-Statistik • bayeskurs</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cerulean/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="Einführung in die Bayes-Statistik">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">bayeskurs</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">0.3.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">Home</a>
</li>
<li>
  <a href="../articles/index.html">Themen</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="http://www.bioimg.statistik.uni-muenchen.de">Bioimaging Group</a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Einführung in die Bayes-Statistik</h1>
                        <h4 class="author">Volker Schmid</h4>
            
            <h4 class="date">30. Mai 2017</h4>
      
      
      <div class="hidden name"><code>einfuehrung.Rmd</code></div>

    </div>

    
    
<div id="thomas-bayes" class="section level1 allowframebreaks">
<h1 class="hasAnchor">
<a href="#thomas-bayes" class="anchor"></a>Thomas Bayes</h1>
<p><img src="pics/Thomas_Bayes.png"> Einziges Bild, vermutlich nicht authentisch}</p>
<p><img src="pics/Image028.jpg"></p>
<div id="thomas-bayes-1" class="section level2">
<h2 class="hasAnchor">
<a href="#thomas-bayes-1" class="anchor"></a>Thomas Bayes</h2>
<ul>
<li>Priester (wie der Vater) und Mathematiker</li>
<li>lebte in Tunbridge Wells (südöstlich von London)</li>
<li>beeinflusst von Abraham de Moivre (frz. Mathematiker, unter anderem Thema Glücksspiel, Satz von de Moivre-Laplace = Grenzwertsatz für Bionomialverteilung)</li>
<li>Drei bekannte Werke: * Göttliche Barmherzigkeit, oder ein Versuch zu beweisen, dass das Ziel der göttlichen Fürsorge und Gewalt das Glück seiner Geschöpfe ist * Eine Einführung in die Lehre der Analysis und eine Verteidigung der Mathematiker gegen die Einwände des Autor von ``The Analyst’’ (George Berkeley, anonym erschienen) * An Essay towards solving a Problem in the Doctrine of Chance (1763)</li>
</ul>
</div>
<div id="kurze-geschichtlicher-uberblick" class="section level2">
<h2 class="hasAnchor">
<a href="#kurze-geschichtlicher-uberblick" class="anchor"></a>Kurze Geschichtlicher Überblick</h2>
<ul>
<li>Mitte 18. Jahrhundert: Bayes entwickelt die Bayes-Formel</li>
<li>Anfang 19. Jahrhundert: Pierre-Simon Laplace entwickelt die Bayes-Formel, prägt den Begriff ‘’Inverse Wahrscheinlichkeit’’</li>
<li>Anfang 20. Jahrhundert: Ronald Fisher entwickelt den Frequentismus, Maximum-Likelihood-Schätzer, prägt den Begriff ‘’Bayes-Statistik’’</li>
<li>Ende des 20. Jahrhunderts: Bayes-Verfahren werden wieder aktuell, komplexe Modelle dank Computer möglich</li>
</ul>
<p>Literatur: Stephen E. Fienberg: When Did Bayesian Inference Become ‘’Bayesian’’? Bayesian Analysis (2006) 1(1), pp. 1–40.*</p>
</div>
</div>
<div id="bayes-und-die-billardkugeln" class="section level1">
<h1 class="hasAnchor">
<a href="#bayes-und-die-billardkugeln" class="anchor"></a>Bayes und die Billardkugeln</h1>
<div id="an-essay-towards-solving-a-problem-in-the-doctrine-of-chance-1763" class="section level2">
<h2 class="hasAnchor">
<a href="#an-essay-towards-solving-a-problem-in-the-doctrine-of-chance-1763" class="anchor"></a>An Essay towards solving a Problem in the Doctrine of Chance (1763)</h2>
<p>Eine weiße Billiardkugel wird auf eine Gerade der Länge 1 gerollt. Die Wahrscheinlichkeit dafür, dass sie an einem Punkt <span class="math inline">\(\pi\)</span> zu liegen kommt, ist konstant für alle <span class="math inline">\(\pi \in [0,1]\)</span>. Eine rote Kugel wird unter den selben Bedinungen <span class="math inline">\(n\)</span>-mal gerollt. Sei <span class="math inline">\(x\)</span> die Zahl der Versuche, in denen die rote Kugel links von der ersten Kugel, also links von <span class="math inline">\(\pi\)</span> zu liegen kommt.</p>
<p>Welche Information über <span class="math inline">\(\pi\)</span> erhalten wir aus der Beobachtung <span class="math inline">\(x\)</span>?</p>
</div>
<div id="billiardkugeln" class="section level2">
<h2 class="hasAnchor">
<a href="#billiardkugeln" class="anchor"></a>Billiardkugeln</h2>
<p>Sei die weiße Kugel bereits gerollt und liege auf dem Punkt <span class="math inline">\(\pi\)</span>. Die rote Kugel gerollt. Dann ist die Wahrscheinlichkeit, dass die rote Kugel links von der weißen zu liegen kommt gleich <span class="math inline">\(\pi\)</span>. Rollen wir <span class="math inline">\(n\)</span>-mal, so handelt es sich um ein Binomialexperiment mit Erfolgswahrscheinlichkeit <span class="math inline">\(\pi\)</span>. Gegeben <span class="math inline">\(\Pi=\pi\)</span> ist also: <span class="math display">\[
P(X=x|\Pi=\pi)=f(x|\pi)={{n}\choose{x}}\pi^x(1-\pi)^{n-x}.
\]</span></p>
<p>Um nun mit dem Satz von Bayes eine Aussage über <span class="math inline">\(pi\)</span> gegeben <span class="math inline">\(x\)</span> zu machen, brauchen wir <span class="math inline">\(f(\pi)\)</span>. Was wissen wir über <span class="math inline">\(\pi\)</span> vor der Beobachtung?</p>
</div>
<div id="priori-und-posteriori" class="section level2">
<h2 class="hasAnchor">
<a href="#priori-und-posteriori" class="anchor"></a>Priori und Posteriori</h2>
<p>Annahme: Vor der Beobachtung, lateinisch , wissen wir nichts über <span class="math inline">\(\pi\)</span>. Wir folgen dem Prinzip von unzureichenden Grund und nehmen <span class="math inline">\(\pi \sim U[0,1]\)</span>.</p>
<p>Dann erhalten wir nach der Beobachtung, lateinisch , mit dem Satz von Bayes <span class="math display">\[\begin{eqnarray}
f(\pi|x)&amp;=&amp;\frac{f(x|\pi)f(\pi)}{\int{f(x|\tilde\pi)f(\tilde\pi)d\tilde\pi}} = \frac{{{n}\choose{x}}\pi^x(1-\pi)^{n-x}\cdot 1}{f(x)} \nonumber\\
&amp;=&amp; C(x) \cdot \pi^x (1-\pi)^{n-x} = C(x) \cdot \pi^{(x+1)-1} (1-\pi)^{(n-x+1)-1} \label{eq:postbilliard}
\end{eqnarray}\]</span> Dabei ist <span class="math inline">\(C(x)\)</span> eine Konstante bezüglich <span class="math inline">\(\pi\)</span> (hängt nicht von <span class="math inline">\(\pi\)</span>, nur von <span class="math inline">\(x\)</span> ab). () sieht bis auf die Konstante aus wie die Dichte der Beta<span class="math inline">\((x+1,n-x+1)\)</span>-Verteilung. Wir sagen: <span class="math inline">\(\pi^{(x+1)-1} \pi^{(n-x+1)-1}\)</span> ist der ‘’Kern’’ der Beta-Verteilung.</p>
</div>
<div id="zusammenfassung" class="section level2">
<h2 class="hasAnchor">
<a href="#zusammenfassung" class="anchor"></a>Zusammenfassung</h2>
<ul>
<li>Wir haben Vorwissen über die Wahrscheinlichkeit <span class="math inline">\(\pi\)</span> in Form einer <strong>Priori-Verteilung</strong> bzw. Priori -Dichte formuliert.</li>
<li>Nach der Beobachtung <span class="math inline">\(x\)</span> wissen wir mehr über <span class="math inline">\(\pi\)</span>; wir haben die  bzw. Posteriori-Dichte erhalten.</li>
<li>Bayes-Prinzip: Alle Schlüsse werden aus der Posteriori-Verteilung gezogen.</li>
<li>Zur Berechnung der Posteriori brauchen wir zudem das Beobachtungsmodell bzw.  <span class="math inline">\(f(x|\pi)\)</span> (auch als Likelihood bezeichnet)</li>
<li>und die  (auch marginale Likelihood), die wir hier nicht explizit berechnen mussten.</li>
</ul>
</div>
<div id="priori-und-posteriori-1" class="section level2">
<h2 class="hasAnchor">
<a href="#priori-und-posteriori-1" class="anchor"></a>Priori und Posteriori</h2>

</div>
</div>
<div id="bayesianische-inferenz" class="section level1">
<h1 class="hasAnchor">
<a href="#bayesianische-inferenz" class="anchor"></a>Bayesianische Inferenz</h1>
<div id="bayesianische-inferenz-1" class="section level2">
<h2 class="hasAnchor">
<a href="#bayesianische-inferenz-1" class="anchor"></a>Bayesianische Inferenz</h2>

</div>
<div id="aufgaben-in-der-bayesianischen-inferenz" class="section level2">
<h2 class="hasAnchor">
<a href="#aufgaben-in-der-bayesianischen-inferenz" class="anchor"></a>Aufgaben in der Bayesianischen Inferenz</h2>

</div>
<div id="bayes-prinzip" class="section level2">
<h2 class="hasAnchor">
<a href="#bayes-prinzip" class="anchor"></a>Bayes-Prinzip</h2>

</div>
<div id="punktschatzer" class="section level2">
<h2 class="hasAnchor">
<a href="#punktschatzer" class="anchor"></a>Punktschätzer</h2>
Billiard-Kugel-Beispiel: sei <span class="math inline">\(n=30\)</span> und <span class="math inline">\(x=5\)</span>. Wie lautet dann unser Schätzer für <span class="math inline">\(\pi\)</span>?

</div>
<div id="kredibilitatsintervall" class="section level2">
<h2 class="hasAnchor">
<a href="#kredibilitatsintervall" class="anchor"></a>Kredibilitätsintervall</h2>
<p>Aus der Posteriori-Verteilung lässt sich ein Intervall <span class="math inline">\([\theta_u, \theta_o]\)</span> bestimmen, das den Parameter <span class="math inline">\(\theta\)</span> mit Wahrscheinlichkeit <span class="math inline">\((1-\alpha)\)</span> enthält.</p>
<p>Ein Intervall <span class="math inline">\(I = [\theta_u, \theta_o]\)</span>, für das gilt <span class="math display">\[ 
\int_{\theta_u}^{\theta_o} p(\theta|x)d\theta = 1-\alpha
\]</span> nennt man <span class="math inline">\((1-\alpha)\)</span>-.</p>
</div>
<div id="hpd-intervall" class="section level2">
<h2 class="hasAnchor">
<a href="#hpd-intervall" class="anchor"></a>HPD-Intervall</h2>
<p>Kredibilitätsintervalle sind nicht eindeutig.</p>
<blockquote>
<p>Ein <span class="math inline">\((1-\alpha)\)</span>-Kredibilitätsintervall <span class="math inline">\(H\)</span> heisst , wenn für alle <span class="math inline">\(\theta \in H\)</span> und alle <span class="math inline">\(\theta^* \not\in H\)</span> gilt: <span class="math display">\[ p(\theta|x) \geq p(\theta^*|x) \]</span></p>
</blockquote>
</div>
<div id="beispiel-kredibilitatsintervalle-betabinomialmodell" class="section level2">
<h2 class="hasAnchor">
<a href="#beispiel-kredibilitatsintervalle-betabinomialmodell" class="anchor"></a>Beispiel: Kredibilitätsintervalle Betabinomialmodell</h2>

<p> \end{Bsp} } </p>
</div>
<div id="pradiktive-posterioriverteilung" class="section level2">
<h2 class="hasAnchor">
<a href="#pradiktive-posterioriverteilung" class="anchor"></a>Prädiktive Posterioriverteilung</h2>
<blockquote>
<p>Die Dichte der  lautet <span class="math display">\[
p(x_Z|x)=\int_\Theta p(x_Z,\theta|x) d\theta = \int p(x_Z|\theta)p(\theta|x) d\theta 
\]</span></p>
</blockquote>
<p>Die prädiktive Posteriori-Verteilung ermöglicht die Prognose von <span class="math inline">\(x\)</span> zum Zeitpunkt <span class="math inline">\(Z\)</span>. Es gilt: <span class="math display">\[\begin{eqnarray*}
E(x_Z|x) &amp;=&amp; E(E(x_Z|\theta,x))\\
Var(x_Z|x)&amp;=&amp; E(Var(x_Z|\theta,x))+Var(E(x_Z|\theta,x))
\end{eqnarray*}\]</span></p>
</div>
<div id="aufgaben-in-der-bayesianischen-inferenz-1" class="section level2">
<h2 class="hasAnchor">
<a href="#aufgaben-in-der-bayesianischen-inferenz-1" class="anchor"></a>Aufgaben in der Bayesianischen Inferenz</h2>
``Bayes-Prinzip: Alle Schlüsse werden <strong>nur</strong> aus der Posteriori-Verteilung gezogen’’ – Was machen wir nun mit der Posteriori?

</div>
</div>
<div id="bayesianische-modellierung" class="section level1">
<h1 class="hasAnchor">
<a href="#bayesianische-modellierung" class="anchor"></a>Bayesianische Modellierung</h1>
<div id="normalverteilungsmodel" class="section level2">
<h2 class="hasAnchor">
<a href="#normalverteilungsmodel" class="anchor"></a>Normalverteilungsmodel</h2>
<p>Gegeben seien <span class="math inline">\(n\)</span> unabhängig normalverteilte Beobachtungen <span class="math display">\[ 
X_i \sim  N(\mu,\sigma^2), i=1,\ldots,n.
\]</span> Die gemeinsame Datendichte lautet <span class="math display">\[\begin{eqnarray*}
f(x|\theta) &amp;=&amp; \prod\left(f(x_i|\theta)\right) \\
&amp;=&amp; \left(\frac{1}{\sigma\sqrt(2\pi)}\right)^n \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2\right)
\end{eqnarray*}\]</span></p>
</div>
<div id="sigma2-bekannt" class="section level2">
<h2 class="hasAnchor">
<a href="#sigma2-bekannt" class="anchor"></a><span class="math inline">\(\sigma^2\)</span> bekannt</h2>
<p>Konjugierte Priori <span class="math inline">\(\mu\sim N(\mu_0,\sigma_0^2)\)</span>. Damit ist die Posteriori <span class="math display">\[\begin{eqnarray*}
\mu|(x_1,\ldots,x_n) &amp;\sim &amp;N(m/s,1/s)\\
m&amp;=&amp;\frac{\mu_0}{\sigma_0^2} + \frac{\sum_{i=1}^n x_i}{\sigma^2}\\
s &amp;=&amp; \frac{1}{\sigma_0^2} + \frac{n}{\sigma^2}
\end{eqnarray*}\]</span></p>
<p>Jeffreys Priori: <span class="math inline">\(p(\mu)\propto\)</span> const. entspricht <span class="math inline">\(\sigma^2_0\to\infty\)</span>.</p>
</div>
<div id="mu-bekannt" class="section level2">
<h2 class="hasAnchor">
<a href="#mu-bekannt" class="anchor"></a><span class="math inline">\(\mu\)</span> bekannt</h2>
<p>Konjugierte Priori: <span class="math inline">\(\sigma^2\sim IG(a,b)\)</span> führt zur Posteriori <span class="math display">\[
\sigma^2|(x_1,\ldots,x_n) \sim IG\left(a+n/2, b+0.5\sum_{i=1}^n{(x_i-\mu)^2}\right)
\]</span></p>
<p>Jeffreys Priori <span class="math inline">\(p(\sigma^2)\propto\sigma^{-1}\)</span> entspricht ‘’IG(0,0)’’.</p>
</div>
<div id="mu-bekannt-alternativ" class="section level2">
<h2 class="hasAnchor">
<a href="#mu-bekannt-alternativ" class="anchor"></a><span class="math inline">\(\mu\)</span> bekannt (Alternativ)</h2>
<p>Alternativ lässt sich die Inferenz für die Präzision gleich Inverse der Varianz betreiben: <span class="math display">\[ \tau=\sigma^{-2}. \]</span> Die konjugierte Priori ist dann die Gamma-Verteilung <span class="math display">\[
p(\tau) = \frac{b^a}{\Gamma(a)}\tau^{a-1}\exp{-b\tau}.
\]</span> Die Posteriori lautet <span class="math display">\[
p(\tau|x)\propto \tau^{n/2}\exp\left(-\tau/2\sum_{i=1}^n(x-\mu)^2\right)\cdot\tau^{a-1}\exp(-b\tau),
\]</span> ist also die Ga(<span class="math inline">\(a+n/2, b+0.5\sum_{i=1}^n{(x_i-\mu)^2}\)</span>)-Verteilung.</p>
</div>
<div id="normalverteilungsmodell-mit-zwei-unbekannten-parametern" class="section level2">
<h2 class="hasAnchor">
<a href="#normalverteilungsmodell-mit-zwei-unbekannten-parametern" class="anchor"></a>Normalverteilungsmodell mit zwei unbekannten Parametern</h2>
<p>Bei jeweils einem unbekanntem Parameter und unter Benutzung der konjugierten Verteilung kennen wir die Posteriori vollständig. Im Folgenden seien beide Parameter (<span class="math inline">\(\mu\)</span> und <span class="math inline">\(\tau\)</span>) unbekannt.</p>
<p>Wir wollen die selben Prioris wie oben benutzen und gehen von -Unabhängigkeit der Parameter aus: <span class="math display">\[ p(\mu,\tau)=p(\mu)\cdot p(\tau) \]</span></p>
<p>Die Posteriori lautet bis auf Konstanten: <span class="math display">\[\begin{eqnarray*}
p(\mu,\tau|x)&amp;\propto&amp; \exp\left(-\tau_0/2 (\mu-\mu_0)^2\right)\\
&amp;&amp;\cdot\tau^{n/2}\exp\left(-\tau/2\sum_{i=1}^n(x-\mu)^2\right)\cdot\tau^{a-1}\exp(-b\tau)
\end{eqnarray*}\]</span></p>
<p>Dabei handelt es sich nicht um eine bekannte zweiparametrische Verteilung.</p>
</div>
<div id="bedingte-posteriori" class="section level2">
<h2 class="hasAnchor">
<a href="#bedingte-posteriori" class="anchor"></a>Bedingte Posteriori</h2>
<p>Wir betrachten die bedingte Posteriori eines Parameters, z.B. <span class="math inline">\(p(\mu|\tau,x)\)</span>. Nach der Definition der bedingten Dichte gilt <span class="math display">\[
p(\mu|\tau,x)=\frac{p(\mu,\tau|x)}{p(\tau|x)}\propto p(\mu,\tau|x)
\]</span> Hier also: Die bedingte Posteriori-Dichte von <span class="math inline">\(\mu\)</span> gegeben <span class="math inline">\(\tau\)</span> ist die Normalverteilung. Das ergibt sich automatisch aus dem Normalverteilungsmodell mit bekannter Varianz!</p>
<p>Die bedingte Posteriori hilft uns aber nicht weiter, weil wir den Parameter <span class="math inline">\(\tau\)</span> nicht kennen.</p>
</div>
<div id="vollstandig-bedingte-posteriori" class="section level2">
<h2 class="hasAnchor">
<a href="#vollstandig-bedingte-posteriori" class="anchor"></a>Vollständig bedingte Posteriori</h2>
<blockquote>
<p>Sei <span class="math inline">\(\bf{\theta}=(\theta_1,\ldots\theta_p)\)</span>. Als vollständig bedingte Posteriori (‘’full conditional posterior’’) bezeichnen wir die Verteilung eines Parameters <span class="math inline">\(\theta_i\)</span> gegeben allen anderen Parametern <span class="math inline">\(\theta_{-i}\)</span> und den Daten <span class="math inline">\(x\)</span>. Es gilt: <span class="math display">\[
p(\theta_i|\theta_{-i},x)\propto p(\bf{\theta}|x).
\]</span></p>
</blockquote>
</div>
<div id="semikonjugierte-priori" class="section level2">
<h2 class="hasAnchor">
<a href="#semikonjugierte-priori" class="anchor"></a>Semikonjugierte Priori</h2>
<p>Eine Familie <span class="math inline">\(\mathcal{F}\)</span> von Verteilungen auf <span class="math inline">\(\Theta\)</span> heißt <strong>semikonjugiert</strong> wenn für jede Priori <span class="math inline">\(p(\theta)\)</span> auf <span class="math inline">\(\mathcal{F}\)</span> die vollständig bedingte Posteriori <span class="math inline">\(p(\theta_i|\theta_{-i},x)\)</span> ebenfalls zu <span class="math inline">\(\mathcal{F}\)</span> gehört.</p>
</div>
<div id="marginale-posteriori" class="section level2">
<h2 class="hasAnchor">
<a href="#marginale-posteriori" class="anchor"></a>Marginale Posteriori</h2>
<p>Wir betrachten die marginale Posteriori eines Parameters, also z.B. <span class="math inline">\(p(\tau|x)\)</span>. Diese erhalten wir durch marginalisieren der gemeinsamen Posteriori <span class="math display">\[ 
p(\tau|x)=\int p(\mu,\tau|x) d\mu. 
\]</span> Alternativ kann man folgende Formel ausnutzen <span class="math display">\[
p(\tau|x)=\frac{p(\mu,\tau|x)}{p(\mu|\tau,x)}
\]</span></p>
<p>Interessiert uns in mehrparametrischen Modellen nur ein Parameter, so ziehen wir die Schlüsse aus der marginalen Posteriori (<em>Model averaging</em>).</p>
</div>
</div>
<div id="regression" class="section level1">
<h1 class="hasAnchor">
<a href="#regression" class="anchor"></a>Regression</h1>
<div id="lineare-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#lineare-regression" class="anchor"></a>Lineare Regression</h2>
<p>Übliches Lineares Regressionsmodell: <span class="math display">\[\begin{eqnarray*}
y_i&amp;=&amp;\alpha+\beta x_i+\epsilon_i\\
{\rm E}(\epsilon)&amp;=&amp;0\\
{\rm Var}(\epsilon)&amp;=&amp;\sigma^2\\
{\rm Cov}(\epsilon_i,\epsilon_j)&amp;=&amp;0\\
\end{eqnarray*}\]</span></p>
</div>
<div id="bayesianisches-lineares-regressionsmodell" class="section level2">
<h2 class="hasAnchor">
<a href="#bayesianisches-lineares-regressionsmodell" class="anchor"></a>Bayesianisches lineares Regressionsmodell</h2>
<span class="math display">\[\begin{eqnarray*}
y_i &amp;\sim &amp; {\rm N}(\alpha+\beta x_i,\sigma^2)\\
\alpha&amp;\sim&amp;{\rm N}(m_\alpha,v_\alpha^2)\\
\beta &amp;\sim&amp;{\rm N}(m_\beta,v_\beta^2)
\end{eqnarray*}\]</span>
<p>Bei festem <span class="math inline">\(\sigma^2\)</span> sind dies die konjugierten Prioris. Wir kennen allerdings <span class="math inline">\(\sigma^2\)</span> in der Regel nicht. Wir nehmen zusätzlich an: <span class="math display">\[
\sigma^2 \sim IG(a,b)
\]</span></p>
<p>#Generalisierte Lineare Regression</p>
</div>
<div id="bayesianisches-generalisiertes-lineares-regressionsmodell" class="section level2">
<h2 class="hasAnchor">
<a href="#bayesianisches-generalisiertes-lineares-regressionsmodell" class="anchor"></a>Bayesianisches generalisiertes lineares Regressionsmodell</h2>
<p>Das Modell lässt sich relativ einfach auf beliebige Verteilungen verallgemeinern, z.B. ein Poisson-Modell <span class="math display">\[\begin{eqnarray*}
y_i &amp;\sim&amp; Po(\lambda_i)\\
\log(\lambda_i) &amp;=&amp; \alpha+\beta x_i\\
\alpha&amp;\sim&amp;{\rm N}(m_\alpha,v_\alpha^2)\\
\beta &amp;\sim&amp;{\rm N}(m_\beta,v_\beta^2)
\end{eqnarray*}\]</span> Die (vollständig bedingten) Posterioris sind jedoch keine Standardverteilungen mehr.</p>
<p><span class="math display">\[ f(y_i|\lambda) = \frac{\lambda_i^{y_i}}{y_i!}\exp{-\lambda_i}\]</span> <span class="math display">\[ \lambda_i=\exp(\alpha+\beta x_i) \]</span></p>
</div>
</div>
<div id="multivariate-regression" class="section level1">
<h1 class="hasAnchor">
<a href="#multivariate-regression" class="anchor"></a>Multivariate Regression</h1>
<div id="multivariate-normal-regression" class="section level2">
<h2 class="hasAnchor">
<a href="#multivariate-normal-regression" class="anchor"></a>Multivariate Normal-Regression</h2>
<span class="math display">\[\begin{eqnarray}
y_i &amp;\sim &amp; {\rm N}(\mathbf{x}_i\boldsymbol\beta,\sigma^2)\\
\sigma^2&amp;\sim&amp;IG(a,b)\\
{\boldsymbol\beta} &amp;\sim&amp;{\rm N}(\mu_0,\boldsymbol\Lambda^{-1}_0)
\end{eqnarray}\]</span>
</div>
<div id="posteriori" class="section level2">
<h2 class="hasAnchor">
<a href="#posteriori" class="anchor"></a>Posteriori</h2>
<span class="math display">\[\begin{eqnarray*}
p(\boldsymbol\beta,\sigma^{2}|\mathbf{y}) &amp;\propto&amp; f(\mathbf{y}|\boldsymbol\beta,\sigma^{2})p(\boldsymbol\beta)p(\sigma^{2})\\
&amp;\propto&amp;  (\sigma^{2})^{-n/2} \exp\left(-\frac{1}{2{\sigma}^{2}}(\mathbf{y}- \mathbf{X} \boldsymbol\beta)^{\rm T}(\mathbf{y}- \mathbf{X} \boldsymbol\beta)\right)\\
&amp;\cdot&amp;     |\boldsymbol\Lambda_0|^{1/2} \exp\left(-\frac{1}{2}(\boldsymbol\beta -\boldsymbol\mu_0)^{\rm T} \boldsymbol\Lambda_0 (\boldsymbol\beta - \boldsymbol\mu_0)\right)\\
&amp;\cdot&amp;  (\sigma^2)^{-(a_0+1)} \exp\left(-\frac{b_0}{{\sigma}^{2}}\right)
\end{eqnarray*}\]</span>
<p><span class="math display">\[ 
p(\beta|\sigma^2,\mathbf{y}) \propto \exp\left(-\frac{1}{2}\boldsymbol\beta^{\rm T}({\sigma}^{2}\mathbf{X}^{\rm T}
\mathbf{X}+\boldsymbol\Lambda_0)\boldsymbol\beta + (\sigma^2\mathbf{y}^{\rm T}\mathbf{X}+\boldsymbol
\mu_0^{\rm T}\boldsymbol \Lambda_0)\boldsymbol \beta)\right) 
\]</span></p>
</div>
<div id="full-conditionals" class="section level2">
<h2 class="hasAnchor">
<a href="#full-conditionals" class="anchor"></a>Full conditionals</h2>
<p>Damit <span class="math display">\[\begin{eqnarray*}
\boldsymbol\beta|\sigma^2,\mathbf{y},\mathbf{x} &amp;\sim&amp; N(\boldsymbol\mu_n,\sigma^2\boldsymbol\Lambda_n^{-1})\\
\sigma^2|\boldsymbol\beta,\mathbf{y},\mathbf{x}&amp;\sim&amp; IG(a_n,b_n)\\
\boldsymbol\mu_n&amp;=&amp;(\sigma^2\mathbf{X}^{\rm T}\mathbf{X}+\boldsymbol\Lambda_0)^{-1} (\boldsymbol\Lambda_0\boldsymbol\mu_0+\sigma^2\mathbf{y}^{\rm T}\mathbf{X}\hat{\boldsymbol\beta})\\
\boldsymbol\Lambda_n&amp;=&amp;(\sigma^2\mathbf{X}^{\rm T}\mathbf{X}+\boldsymbol\Lambda_0) \\
a_n&amp;=&amp;a_0+\frac{n}{2}\\
b_n&amp;=&amp;b_0+\frac{1}{2}(\mathbf{y}^{\rm T}\mathbf{y}+\boldsymbol\mu_0^{\rm T}\boldsymbol\Lambda_0\boldsymbol\mu_0-\boldsymbol\mu_n^{\rm T}\boldsymbol\Lambda_n\boldsymbol\mu_n) 
\end{eqnarray*}\]</span></p>
</div>
<div id="random-walk-priori" class="section level2">
<h2 class="hasAnchor">
<a href="#random-walk-priori" class="anchor"></a>Random Walk Priori</h2>
<p>Über die Kovarianz- oder die Präzisionsmatrix lassen sich Korrelationen zwischen den Kovariableneffekten modellieren. Z.B. ein zeitlich geglätterer Trend.</p>
<p><strong>Beispiel: Random Walk</strong> Gegeben sei eine Zeitreihe <span class="math inline">\(y_t\)</span> mit <span class="math inline">\(t=1,\ldots,T\)</span>. Wir wollen die Zeitreihe glätten. Sei <span class="math inline">\(\mathbf{X}=\mathbf{I}_T\)</span>, dann ist obiges Modell gleich <span class="math display">\[y_t =\beta_t + \epsilon_t \text{ für }t=1,\ldots,T\]</span> Als Priori für <span class="math inline">\(\beta\)</span> nehmen wir einen ‘’Random Walk’’: <span class="math display">\[\begin{eqnarray*}
\beta_1 &amp;\sim &amp;N(0,\tau_0^2)\\
\beta_t &amp;\sim &amp;N(\beta_{t-1},\tau^2)\\
\end{eqnarray*}\]</span> Der Parameter <span class="math inline">\(\tau\)</span> steuert die  der Zeitreihe <span class="math inline">\(\beta_t\)</span>.</p>
</div>
<div id="prazisionsmatrix" class="section level2">
<h2 class="hasAnchor">
<a href="#prazisionsmatrix" class="anchor"></a>Präzisionsmatrix</h2>
<p>Es lässt sich zeigen (mit <span class="math inline">\(\tau_0\to \infty\)</span>): <span class="math display">\[ \Lambda = \tau^{-2}\left(\begin{array}{ccccccc}
1 &amp; -1 &amp; 0 &amp; \cdots &amp; &amp; &amp; 0\\
-1 &amp; 2 &amp; -1 &amp; 0 &amp; \cdots &amp; &amp; 0\\
0 &amp; -1 &amp; 2 &amp; -1 &amp; 0 &amp; \cdots &amp; 0\\
\vdots &amp; &amp; &amp; \ddots &amp; \ddots&amp; \ddots &amp; \vdots \\
&amp; &amp;  \cdots &amp; 0 &amp; -1 &amp; 2 &amp; -1 \\
&amp; &amp; &amp; \cdots &amp; 0 &amp; -1 &amp; 1
\end{array}\right) \]</span></p>
<p>!(pics/globalwarming-rw.png)</p>
</div>
</div>
<div id="hierarchische-modellierung" class="section level1">
<h1 class="hasAnchor">
<a href="#hierarchische-modellierung" class="anchor"></a>Hierarchische Modellierung</h1>
<div id="hierarchische-bayesianische-modelle" class="section level2">
<h2 class="hasAnchor">
<a href="#hierarchische-bayesianische-modelle" class="anchor"></a>Hierarchische Bayesianische Modelle</h2>
<ul>
<li>Level 1: Datenmodell, Definition der Likelihood</li>
<li>Level 2: Priori-Modell der unbekannten Parameter</li>
<li>Level 3: (Hyper-)Prioris der Prioriparameter in Level 2</li>
</ul>
<p>Kann an sich beliebig erweitert werden, i.d.R. reichen aber drei Level. Inferenz üblicherweise mit MCMC.</p>
</div>
<div id="beispiel-raumliches-apc-modell" class="section level2">
<h2 class="hasAnchor">
<a href="#beispiel-raumliches-apc-modell" class="anchor"></a>Beispiel: Räumliches APC-Modell</h2>
<p>Anzahl männliche Magenkrebstote in Westdeutschland</p>
<ul>
<li>Jahre 1976 - 1990</li>
<li>13 Altersgruppen a 5 Jahre (15-19 bis 85-89)</li>
<li>Geburtskohorten von 1896-1975</li>
<li>30 Regierungsbezirke</li>
</ul>
</div>
<div id="hierarchisches-bayes-modell-level-1" class="section level2">
<h2 class="hasAnchor">
<a href="#hierarchisches-bayes-modell-level-1" class="anchor"></a>Hierarchisches Bayes-Modell: Level 1</h2>
<p><span class="math display">\[ \begin{aligned}
y_{ijt} &amp;\sim \mbox{B}(n_{ijt},\pi_{ijt})\\
\mbox{logit}(\pi_{jtl}) &amp;=&amp; \xi_{jtl} \\ 
&amp;= \mu + \theta_j + \phi_t + \psi_k + \alpha_l + \left[
\begin{array}{c}
\delta_{jl} \\
\delta_{kl}
\end{array}
\right] + z_{jtl}
\end{aligned} \]</span></p>
</div>
<div id="hierarchisches-bayes-modell-parameter" class="section level2">
<h2 class="hasAnchor">
<a href="#hierarchisches-bayes-modell-parameter" class="anchor"></a>Hierarchisches Bayes-Modell: Parameter</h2>
<ul>
<li>
<span class="math inline">\(\mu\)</span>: Intercept (1 Parameter)</li>
<li>
<span class="math inline">\(\theta_j\)</span>: Effekt der Altersgruppe <span class="math inline">\(j\)</span> (15 Parameter)</li>
<li>
<span class="math inline">\(\phi_t\)</span>: Effekt der Periode <span class="math inline">\(t\)</span> (13 Parameter)</li>
<li>
<span class="math inline">\(\psi_k\)</span>: Effekt der Kohorte <span class="math inline">\(k=k(j,t)\)</span> (75 Parameter)</li>
<li>
<span class="math inline">\(\alpha_l\)</span>: Räumlicher Effekt <span class="math inline">\(l\)</span> (30 Parameter)</li>
<li>
<span class="math inline">\(\delta_{tl}\)</span>: Interaktion zwischen Perioden- und räumlichen Effekt (390 Parameter)</li>
<li>
<span class="math inline">\(z_{jtl}\)</span>: zufälliger Effekt (Überdispersion, 5850 Parameter)</li>
</ul>
</div>
<div id="hierarchisches-bayes-modell-level-2" class="section level2">
<h2 class="hasAnchor">
<a href="#hierarchisches-bayes-modell-level-2" class="anchor"></a>Hierarchisches Bayes-Modell: Level 2</h2>
<ul>
<li>Random Walk Priori für APC-Effekte mit Glättungsparameter (Präzision)</li>
<li>2D-Random-Walk (Gauss-Markovzufallsfeld) für räumlichen Effekt mit Glättungsparameter</li>
<li>Interaktion: Unabhängiger Random Walk pro Region oder 3D-GMZF mit Glättungsparameter</li>
<li>iid normalverteilter zufälliger Effekt mit unbekannter Varianz/Präzision</li>
</ul>
<p>Alle Prioris haben die Form <span class="math display">\[
p({\bf \theta}|\kappa) \propto \exp \left(- \frac{\kappa}{2} \, {\bf \theta}^T {\bf K}_{\theta}
{\bf \theta} \right)
\]</span></p>
</div>
<div id="hierarchisches-bayes-modell-level-3" class="section level2">
<h2 class="hasAnchor">
<a href="#hierarchisches-bayes-modell-level-3" class="anchor"></a>Hierarchisches Bayes-Modell: Level 3</h2>
<p>Gamma-Prioris auf alle Präzisionsparameter. Hyperprioriparameter können Ergebnis beeinflußen <span class="math inline">\(\to\)</span> Sensitivitätsanalyse.</p>
<ul>
<li>Sehr komplexe Posteriori</li>
<li>Marginale Posterioris nicht geschlossen herleitbar</li>
<li>Bedingte Posterioris leicht anzugeben und (mit Trick) Standardverteilungen</li>
</ul>
</div>
<div id="ergebnisse" class="section level2 allowframebreaks">
<h2 class="hasAnchor">
<a href="#ergebnisse" class="anchor"></a>Ergebnisse</h2>
<p><img src="pics/sapc-apc.png"></p>
<p><img src="pics/sapc-interaktionen.png"></p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li>
<a href="#thomas-bayes">Thomas Bayes</a><ul class="nav nav-pills nav-stacked">
<li><a href="#thomas-bayes-1">Thomas Bayes</a></li>
      <li><a href="#kurze-geschichtlicher-uberblick">Kurze Geschichtlicher Überblick</a></li>
      </ul>
</li>
      <li>
<a href="#bayes-und-die-billardkugeln">Bayes und die Billardkugeln</a><ul class="nav nav-pills nav-stacked">
<li><a href="#an-essay-towards-solving-a-problem-in-the-doctrine-of-chance-1763">An Essay towards solving a Problem in the Doctrine of Chance (1763)</a></li>
      <li><a href="#billiardkugeln">Billiardkugeln</a></li>
      <li><a href="#priori-und-posteriori">Priori und Posteriori</a></li>
      <li><a href="#zusammenfassung">Zusammenfassung</a></li>
      <li><a href="#priori-und-posteriori-1">Priori und Posteriori</a></li>
      </ul>
</li>
      <li>
<a href="#bayesianische-inferenz">Bayesianische Inferenz</a><ul class="nav nav-pills nav-stacked">
<li><a href="#bayesianische-inferenz-1">Bayesianische Inferenz</a></li>
      <li><a href="#aufgaben-in-der-bayesianischen-inferenz">Aufgaben in der Bayesianischen Inferenz</a></li>
      <li><a href="#bayes-prinzip">Bayes-Prinzip</a></li>
      <li><a href="#punktschatzer">Punktschätzer</a></li>
      <li><a href="#kredibilitatsintervall">Kredibilitätsintervall</a></li>
      <li><a href="#hpd-intervall">HPD-Intervall</a></li>
      <li><a href="#beispiel-kredibilitatsintervalle-betabinomialmodell">Beispiel: Kredibilitätsintervalle Betabinomialmodell</a></li>
      <li><a href="#pradiktive-posterioriverteilung">Prädiktive Posterioriverteilung</a></li>
      <li><a href="#aufgaben-in-der-bayesianischen-inferenz-1">Aufgaben in der Bayesianischen Inferenz</a></li>
      </ul>
</li>
      <li>
<a href="#bayesianische-modellierung">Bayesianische Modellierung</a><ul class="nav nav-pills nav-stacked">
<li><a href="#normalverteilungsmodel">Normalverteilungsmodel</a></li>
      <li><a href="#sigma2-bekannt"><span class="math inline">\(\sigma^2\)</span> bekannt</a></li>
      <li><a href="#mu-bekannt"><span class="math inline">\(\mu\)</span> bekannt</a></li>
      <li><a href="#mu-bekannt-alternativ"><span class="math inline">\(\mu\)</span> bekannt (Alternativ)</a></li>
      <li><a href="#normalverteilungsmodell-mit-zwei-unbekannten-parametern">Normalverteilungsmodell mit zwei unbekannten Parametern</a></li>
      <li><a href="#bedingte-posteriori">Bedingte Posteriori</a></li>
      <li><a href="#vollstandig-bedingte-posteriori">Vollständig bedingte Posteriori</a></li>
      <li><a href="#semikonjugierte-priori">Semikonjugierte Priori</a></li>
      <li><a href="#marginale-posteriori">Marginale Posteriori</a></li>
      </ul>
</li>
      <li>
<a href="#regression">Regression</a><ul class="nav nav-pills nav-stacked">
<li><a href="#lineare-regression">Lineare Regression</a></li>
      <li><a href="#bayesianisches-lineares-regressionsmodell">Bayesianisches lineares Regressionsmodell</a></li>
      <li><a href="#bayesianisches-generalisiertes-lineares-regressionsmodell">Bayesianisches generalisiertes lineares Regressionsmodell</a></li>
      </ul>
</li>
      <li>
<a href="#multivariate-regression">Multivariate Regression</a><ul class="nav nav-pills nav-stacked">
<li><a href="#multivariate-normal-regression">Multivariate Normal-Regression</a></li>
      <li><a href="#posteriori">Posteriori</a></li>
      <li><a href="#full-conditionals">Full conditionals</a></li>
      <li><a href="#random-walk-priori">Random Walk Priori</a></li>
      <li><a href="#prazisionsmatrix">Präzisionsmatrix</a></li>
      </ul>
</li>
      <li>
<a href="#hierarchische-modellierung">Hierarchische Modellierung</a><ul class="nav nav-pills nav-stacked">
<li><a href="#hierarchische-bayesianische-modelle">Hierarchische Bayesianische Modelle</a></li>
      <li><a href="#beispiel-raumliches-apc-modell">Beispiel: Räumliches APC-Modell</a></li>
      <li><a href="#hierarchisches-bayes-modell-level-1">Hierarchisches Bayes-Modell: Level 1</a></li>
      <li><a href="#hierarchisches-bayes-modell-parameter">Hierarchisches Bayes-Modell: Parameter</a></li>
      <li><a href="#hierarchisches-bayes-modell-level-2">Hierarchisches Bayes-Modell: Level 2</a></li>
      <li><a href="#hierarchisches-bayes-modell-level-3">Hierarchisches Bayes-Modell: Level 3</a></li>
      <li><a href="#ergebnisse">Ergebnisse</a></li>
      </ul>
</li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Volker Schmid.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
