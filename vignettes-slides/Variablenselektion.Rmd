---
title: "Bayesianische Variablenselektion"
author: "Volker Schmid"
date: "10. Juli 2017"
output:
  beamer_presentation:
    keep_tex: false
    toc: true
    slide_level: 2
vignette: >
  %\\VignetteIndexEntry{Bayesianische Variablenselektion}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(bayeskurs)
```

# Ridge und Lasso

## Bayesianisches (Generalisiertes) Lineares Modell

Gegeben seien $n$ Beobachtungen einer Zielvariable $y$ und von $p$ Kovariablen $x_1,\ldots,x_p$.

$$ \begin{aligned}
y_i|\mu_i,\phi_i &\sim f(\mu_i,\phi_i) \text{ i.i.d.}\\
h(\mu_i) & = \sum_{j=1}^p \beta_j x_{ij} 
\end{aligned}$$

## Bayesianisches lineares Regressionsmodell

$$\begin{aligned}
y_i|\beta,\sigma^2 & \sim {\rm N}(\mu_i,\sigma^2)\\
\mu_i & = \sum_{j=1}^p \beta_j x_{ij}
\end{aligned}$$

Welche Priori-Information haben wir über die $\beta$? Erstmal keine...

$$p(\beta_j) \propto \text{const.} $$
Können wir sehen als uneigentliche Normalverteilung (hier konjugierte Verteilung) mit Varianz unendlich. Damit ist

$$ \beta\sim N_p(\hat{\beta},\Sigma)$$

mit $\hat{\beta} = (X'X)^{-1}X'y$ dem KQ-Schätzer!


## Ridge-Regression {.allowframebreaks}

Nun: $p>>n$. 

Idee der Ridge-Regression: Viele der $\beta$ Parameter sollen gleich oder nahe Null sein. Bestrafe daher Parameter, die zu stark von der Null abweichen. Damit penalisierter log-Likelihood-Ansatz:

$$ l_{pen}(\beta)=l(\beta)-\frac{\lambda}{2} \sum_{j=1}^p \beta_j^2 $$

Bayesianisch gedacht: Wir haben die Vorinformation, dass die Parameter nahe Null sind. Kombiniert mit konjugiertem Priori-Ansatz kommen wir auf:


$$ \beta_j \sim N(0,\tau^{-1}) \quad \forall j $$

Die Log-Priori-Dichte ist 
$$ \log(p(\beta))=-\frac{\tau}{2}\sum_{j=1}^p \beta^2 +C $$

Damit sind penalisierte log-Likelihood und log-Posteriori bis auf Konstanten identisch. Ein Maximum-A-Posteriori-Ansatz liefert also selbes Ergebnis wie ein penalisierter log-Likelihood-Ansatz (Tikhonov-Regularisierung).

## Beispiel Bayesianische Ridge-Regression {.allowframebreaks}

Wir konstruieren einen Beispieldatensatz
```{r ridge-daten}
n <- 50 
p <- 100
true.sigma2 <- 0.001
x <- matrix(runif(n*p), nrow=n)
true.beta <- c(10,20,30, rep(0,p-3))
mu <- as.vector(x%*%true.beta)
y <- rnorm(n,mu,sqrt(true.sigma2))
par(mfrow=c(2,3))
for (i in c(1:4,10,90))
  plot(x[,i],y)
```

## Posteriori

$$ \begin{aligned}
p(\beta,\tau,\sigma^2|y) &\propto \sigma^{-n}\exp\left(-\frac{1}{2\sigma^2}\sum_i(y_i-\sum_j\beta_jx_{ij})^2\right)\\
&\cdot \tau^{p/2}\exp\left(-\frac{\tau}{2}\sum_j\beta_j^2\right)\\
& \cdot \tau^{a-1}\exp(-\tau b)\\
& \cdot \sigma^2{-a_0-1}\exp(-b_0/\sigma^2)\\
\end{aligned} $$

Damit gilt:

* $\beta|\tau,\sigma^2 \sim N(\hat{\beta},\Sigma)$
mit 
$\hat{\beta}=(X'X+\tau I)^{-1}X'y$ und $\Sigma=(X'X+\tau I)^{-1})$.
* $\tau|\beta \sim Ga(a+p/2,b+\sum\beta_j^2/2)$
* $\sigma^2|\beta,y \sim IG(a_0+n/2,b+\sum(\epsilon_i^2))$ mit $\epsilon_i=y_i-\sum_j\beta_jx_{ij}$ 

## MMCM {.allowframebreaks}

```{r ridge-mcmc-prep}
beta<-rep(0,p)
XX <- t(x)%*%x
Xy <- t(x)%*%y
tau <- 1
sigma2 <- 1
a0 <- 1
b0 <- 0.001
a <- 1
b <- 0.1

beta.save<-array(NA,c(p,500))
tau.save<-rep(NA,500)
sigma2.save<-rep(NA,500)
```

```{r ridge-mcmc}
for (i in 1:1000)
{
  Sigma <- solve(XX+tau*diag(p))
  mu <- Sigma%*%Xy
  beta <- mnormt::rmnorm(1,mu,Sigma)
  
  tau <- rgamma(1, a+p/2, b+sum(beta^2)/2)

  sigma2 <- 1/rgamma(1, a0+n/2, b0+sum((y-x%*%beta)^2))
  
  if (i>500)
  {
    beta.save[,i-500]=beta
    tau.save[i-500]=tau
    sigma2.save[i-500]=sigma2
  }
}
```

## Plot

```{r ridge-plot}
beta.qu<-apply(beta.save,1,quantile,probs=c(.05,.5,.95))
plot(beta.qu[2,],pch=19,ylim=range(beta.qu),ylab="beta")
for (i in 1:p)
lines(rep(i,2),beta.qu[c(1,3),i],lwd=2)
```

## Relevante Kovariablen

```{r ridge-find-beta-0}
sum(beta.qu[2,]==0)
print(which(beta.qu[1,]>0))
print(which(beta.qu[3,]<0))
```

* Bei Ridge werden die Parameter Richtung Null gedrückt
* Aber: Parameter werden nicht genau gleich Null!

## Lasso


Alternative: Lasso ($L_1$-Regularisierung)

$$ pen(\beta)=\sum_j|\beta_j| $$

Bayesianisch analog zu Ridge:

$$ p(\beta_j) \propto \exp\left(-\frac{\tau}{2} \sum |\beta_j| \right)$$

* Das entspricht einer Laplace-Verteilung mit Erwartungswert 0
* Aber: (erstmal) kein Gibbs-Sampler mehr möglich

## Bayesianischer Lasso {.allowframebreaks}

Nach *Park, Trevor and Casella, George. The Bayesian Lasso. Journal of American Statistical Association. 103(482):681-686. 2008* gilt äquivalent:

$$ \begin{aligned}
\beta_j | \sigma^2,\tau_j^2 &\sim N(0,\sigma^2\tau_j^2)\\
\tau_j|\sigma^2 & \sim Exp(\lambda^2/2)\\
\lambda^2 &\sim Ga(a,b)
\end{aligned} $$

Damit läßt sich wiederum ein Gibbs-Sampler konstruieren.

```{r lasso}
beta.L<-gibbsBLasso(x, y, max.steps = 10000)
plot(beta.L[2,],pch=19,ylim=range(beta.L),ylab="beta")
for (i in 1:p)
lines(rep(i,2),beta.L[c(1,3),i],lwd=2)

sum(beta.L[2,]==0)
print(which(beta.L[1,]>0))
print(which(beta.L[3,]<0))
```

## Elastic Net

Ridge und Lasso lassen sich kombinieren: 

$$ p(\beta_j) \propto \exp\left(-\frac{\tau}{2} \sum |\beta_j| -\frac{\nu}{2} \sum \beta_j^2 \right)$$

```{r priorplot, echo=FALSE}
x<-seq(-5,5,length=1000)
ridge <- dnorm(x)
lasso <- VGAM::dlaplace(x)
enet <- 0.5*(ridge+lasso)
plot(x, lasso, type="l", ylab="", xlab="beta")
lines(x, ridge, col="blue")
lines(x, enet, col="green")
legend(3.5,0.5,col=c("blue","black","green"),legend=c("ridge","lasso","enet"),pch=19)
```

# Indikatorvariablen

## Indikatorvariablen

Setze

$$\beta_i = I_i\tilde{\beta}_i $$

wobei $I_i$ eine (0/1-)Indikatorvariable ist. 

Ist $I_i=0$, wird $\beta_i$ auf $0$ gesetzt, $\tilde{\beta}_i$ wird aus der Priori gezogen.

## Ising-Feld 

* Ansatz lässt sich auch auf Kovariablen mit bekannter/angenommener Korrelation anwenden
* Z.B. Gene auf DNA, Bilder
* Auf die Indikatorvariablen wird das ein Ising-Feld angenommen
* mit $J_i=2I_i-1$, also $J_i\in\{-1,1\}$

$$ p(I) \propto \exp\left(-\tau \sum_{i\sim j}J_i J_j\right) $$

* Sampling daraus allerdings schwierig
* Alternative: Probit-Modell

$$ I_i = \left\{ \begin{aligned}
1 & \text{ für } \phi>0\\
0 & \text{ für } \phi\leq 0 \end{aligned}\right. 
$$

mit 

$$p(\phi) \propto \left(\exp(-\frac{\tau}{2}\sum_{i\sim j}(\phi_i-\phi_j)^2\right)$$

# Spike and Slab

## Spike and Slab {.allowframebreaks}

* Idee: Damit $p(\beta=0|y)>0$, muss $p(\beta=0)>0$ sein
* Kombiniere flache Priori (Slab) mit Punktmasse auf Null (Spike)  
* Computational bessere Darstellung als Mischung von zwei Normalverteilungen mit sehr großer uns sehr kleiner Varianz

```{r spikeslabplot, echo=FALSE}
x<-seq(-5,5,length=1000)
p <- 0.5*(dnorm(x,0,.1)+dnorm(x,0,1))
plot(x, p, type="l", ylab="", xlab="beta")
```

* Änhlichkeiten zu Elastic Net, wenn Lasso über Normalverteilung modelliert wird.
* z.B. Implementation in spikeSlabGAM-Paker

$$ \begin{aligned}
\beta|\gamma,\tau^2 &\sim N(0,\tau^2\gamma)\\
\gamma|w & \sim wI_1(\gamma) + (1-w)I_{\nu_0}(\gamma)\\
\tau^2&\sim IG(a_\tau, b_\tau)\\
w&\sim Beta(a_w, b_w)
\end{aligned}$$

* $\nu_0$ sehr klein, entspricht *spike*

## WGRR und gen

* Ishwaran und Rao (2014) zeigen, dass die Spike and Slab prior ein Spezialfall der gewichteten generalisierten Ridge-Regression sind (weighted generalized Ridge regression, WGRR)
* Beim WGRR können die $\beta$ Parameter auf Null gesetzt werden, wir erhalten $p(\beta=0|y)$
* Durch Bayesian Model Averaging gehen aber z.B. bei $E(y^*|y)$ weiterhin viele/alle Regressionsparameter ein
* Ishwaran und Rao (2014) schlagen weiterhin das generalizes elatic net (gen) vor, das mehr Paramete auf Null setzt

## spikeslabGAM paket {.allowframebreaks}

Beim spikeSlabGAM-Paket wird die Variablenselektion auf die glatten Effekte angewandt:

```{r spikeslabgam-data, cache=TRUE, echo=FALSE}
library("spikeSlabGAM")
data("PimaIndiansDiabetes2", package = "mlbench")
pimaDiab <- na.omit(PimaIndiansDiabetes2[, -c(4, 5)])
pimaDiab <- within(pimaDiab,{
            diabetes <- 1*(diabetes=="pos")
        })
testInd <- sample(1:nrow(pimaDiab), 200)
pimaDiabTrain <- pimaDiab[-testInd,]  
 plot(pimaDiabTrain)
 ```

```{r spikeslabgam-mcmc, cache=TRUE}
mcmc <- list(nChains=4, chainLength=1000, burnin=500, thin=5)
m0 <- spikeSlabGAM(diabetes ~ pregnant + glucose + pressure + mass + pedigree + age,
        family="binomial", data=pimaDiabTrain, mcmc=mcmc)
```

```{r spikeslabgam-plots}
print(summary(m0), printModels=FALSE)
```